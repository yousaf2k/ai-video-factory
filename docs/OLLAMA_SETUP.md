# Ollama LLM Setup Guide

## Overview

Ollama is a **local, open-source LLM platform** that runs entirely on your machine. It offers:
- **Zero API costs** - Free after hardware investment
- **Complete privacy** - No data leaves your machine
- **Offline operation** - Works without internet
- **Open-source models** - Llama 2, 3, Mistral, Qwen, and more

## Why Use Ollama?

### Advantages ‚úÖ
- **Privacy**: All prompts and responses stay local
- **Cost**: Free (no API fees)
- **Offline**: Works without internet connection
- **Custom models**: Use finetuned or private models
- **No API keys**: Simplified setup

### Disadvantages ‚ùå
- **Hardware dependent**: Requires decent CPU/GPU
- **Setup complexity**: Must install and run server separately
- **Model management**: You download and manage models
- **Speed**: Depends on your hardware

## Prerequisites

### Hardware Requirements

**Minimum** (CPU-only):
- CPU: 4+ cores
- RAM: 8GB
- Storage: 10GB per model

**Recommended** (GPU):
- GPU: NVIDIA GPU with 6GB+ VRAM
- RAM: 16GB
- Storage: SSD with 20GB+ space

### Operating System
- Windows 10/11
- macOS
- Linux

## Installation

### Step 1: Download Ollama

1. Visit: https://ollama.com
2. Click "Download" for your OS
3. Run installer

### Step 2: Verify Installation

Open terminal/command prompt:

```bash
# Check Ollama version
ollama --version

# Expected output: ollama version is 0.1.x
```

### Step 3: Start Ollama Server

**Option A: Run in Background** (Recommended for development)

```bash
# Windows (PowerShell)
Start-Process ollama "serve"

# Windows (CMD)
start /b ollama serve

# macOS/Linux
ollama serve &
```

**Option B: Run in Foreground** (For testing)

```bash
ollama serve
```

Server runs at: `http://localhost:11434`

### Step 4: Download a Model

```bash
# List available models
ollama list

# Download popular models
ollama pull llama2           # Meta's Llama 2 (7B parameters)
ollama pull mistral           # Mistral 7B
ollama pull qwen2             # Qwen 2 (Alibaba)
ollama pull codellama         # Code-optimized Llama
ollama pull llama3            # Llama 3 (latest)

# Verify download
ollama list
```

**Model Recommendations**:
- **llama2** (7B): Good balance of speed/quality
- **mistral** (7B): Fast, good quality
- **qwen2** (7B): Great for Chinese/English
- **codellama**: For code generation tasks
- **llama3** (8B): Latest, best quality (slower)

### Step 5: Test Ollama

```bash
# Test generation
ollama run llama2 "Explain quantum computing in one sentence"

# Expected: AI generates response
```

## Configuration

### Configure AI Video Factory

Edit `config.py`:

```python
# Use Ollama as LLM provider
LLM_PROVIDER = "ollama"

# Ollama server URL (default: localhost)
OLLAMA_BASE_URL = "http://localhost:11434"

# Model to use (must match downloaded model)
OLLAMA_MODEL = "llama2"  # or mistral, qwen2, codellama, etc.
```

### Environment Variables (Alternative)

```bash
# Windows PowerShell
$env:LLM_PROVIDER = "ollama"
$env:OLLAMA_BASE_URL = "http://localhost:11434"
$env:OLLAMA_MODEL = "llama2"

# Windows CMD
set LLM_PROVIDER=ollama
set OLLAMA_BASE_URL=http://localhost:11434
set OLLAMA_MODEL=llama2

# Linux/macOS
export LLM_PROVIDER=ollama
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_MODEL=llama2
```

## Testing

### Test 1: Verify Server Running

```bash
# Check if Ollama is accessible
curl http://localhost:11434/api/version

# Expected: Returns version JSON
```

### Test 2: Test Provider Factory

```bash
cd C:\AI\ai_video_factory
python -c "from core.llm_engine import get_provider; p = get_provider('ollama'); print(f'Provider: {p.name}, Model: {p.model}')"

# Expected output: Provider: Ollama, Model: llama2
```

### Test 3: Test Story Generation

```bash
# Set provider
set LLM_PROVIDER=ollama
set OLLAMA_MODEL=llama2

# Test generation
python -c "from core.story_engine import build_story; print(build_story('A cat dancing')[0:200])"

# Expected: Story generated by Ollama
```

## Troubleshooting

### "Connection refused" Error

**Cause**: Ollama server not running

**Solution**:
```bash
# Start server
ollama serve

# Or run in background (Windows)
Start-Process ollama "serve"
```

### "Model not found" Error

**Cause**: Model not downloaded

**Solution**:
```bash
# List available models
ollama list

# Download missing model
ollama pull llama2
```

### Slow Generation

**Cause**: CPU-only inference

**Solutions**:
1. **Use smaller model**: `mistral` instead of `llama2`
2. **Enable GPU**: Install NVIDIA CUDA drivers
3. **Reduce prompt length**: Shorter prompts = faster
4. **Use faster model**: `qwen2` or `mistral` over `llama2`

### Out of Memory

**Cause**: Model too large for RAM/VRAM

**Solutions**:
1. **Use smaller model**: 7B instead of 70B
2. **Close other applications**: Free up RAM
3. **Adjust context length**: Reduce prompt/response size

### Port Already in Use

**Cause**: Another process using port 11434

**Solution**:
```bash
# Windows - Find and kill process
netstat -ano | findstr :11434
taskkill /PID <PID> /F

# Or use different port
$env:OLLAMA_BASE_URL = "http://localhost:11435"
```

## Performance Tips

### GPU Acceleration

1. **Install NVIDIA drivers**: https://www.nvidia.com/Download/index.aspx
2. **Install CUDA**: Ollama uses CUDA automatically
3. **Verify GPU usage**: Check Task Manager/nvidia-smi during generation

### Model Selection

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **mistral** | 7B | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Fast iteration |
| **qwen2** | 7B | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Balanced |
| **llama2** | 7B | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | General use |
| **codellama** | 7B | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Code tasks |
| **llama3** | 8B | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Highest quality |

### Hardware Optimization

**For CPU-only**:
- Use 7B parameter models (mistral, qwen2)
- Close background applications
- Use SSD for model storage

**For GPU**:
- Use larger models (13B, 70B) if VRAM allows
- Monitor GPU usage with `nvidia-smi`
- Adjust batch size for optimal throughput

## Advanced Configuration

### Custom Base URL

If Ollama running on remote server:

```python
# config.py
OLLAMA_BASE_URL = "http://192.168.1.100:11434"  # Remote server
```

### Model-Specific Configuration

Different models for different tasks:

```python
# For story generation (quality)
OLLAMA_MODEL = "llama3"

# For rapid prototyping (speed)
OLLAMA_MODEL = "mistral"

# For code-related tasks
OLLAMA_MODEL = "codellama"
```

### Context Window

Adjust maximum context length (if supported by model):

```bash
# Run with custom context
ollama run llama2 --ctx-size 4096
```

## Model Library

Popular models to download:

```bash
# Meta Models
ollama pull llama2              # Llama 2 (7B)
ollama pull llama3              # Llama 3 (8B)
ollama pull codellama           # Code Llama

# Mistral AI
ollama pull mistral             # Mistral 7B
ollama pull mixtral             # Mixtral 8x7B

# Alibaba
ollama pull qwen2               # Qwen 2 (7B)

# Others
ollama pull phi                 # Microsoft Phi (3.8B)
ollama pull tinyllama           # TinyLlama (1.1B) - Fastest!
ollama pull neural-chat         # Chat-optimized
```

## API Reference

### Ollama API Endpoints

**Generate Text**:
```
POST /api/generate
{
  "model": "llama2",
  "prompt": "Your prompt here",
  "stream": false
}
```

**List Models**:
```
GET /api/tags
```

**Version Info**:
```
GET /api/version
```

## Comparison with Cloud Providers

| Feature | Ollama | Gemini/OpenAI |
|---------|--------|--------------|
| **Cost** | Free (hardware) | Pay-per-token |
| **Privacy** | ‚≠ê‚≠ê‚≠ê (local) | ‚≠ê‚≠ê (cloud) |
| **Speed** | Hardware-dependent | Network-dependent |
| **Setup** | Moderate | Easy |
| **Offline** | ‚úÖ Yes | ‚ùå No |
| **API Key** | ‚ùå Not needed | ‚úÖ Required |

## Best Practices

### When to Use Ollama ‚úÖ

- **Development/testing**: Cost-free iteration
- **Sensitive content**: Privacy required
- **Offline environments**: No internet available
- **Custom models**: Finetuned models needed
- **Cost optimization**: Zero API costs

### When to Use Cloud Providers ‚òÅÔ∏è

- **Production**: Need reliability/speed
- **No GPU**: Limited local hardware
- **Best quality**: State-of-the-art models
- **Simple setup**: Don't want to manage servers

## Next Steps

1. ‚úÖ Install Ollama
2. ‚úÖ Download model (`ollama pull llama2`)
3. ‚úÖ Start server (`ollama serve`)
4. ‚úÖ Configure AI Video Factory (`config.py`)
5. ‚úÖ Test generation

**Ready to use Ollama!** üéâ

For more information:
- Ollama Website: https://ollama.com
- Ollama GitHub: https://github.com/ollama/ollama
- Model Library: https://ollama.com/library
