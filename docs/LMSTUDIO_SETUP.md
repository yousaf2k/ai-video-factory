# LM Studio LLM Setup Guide

## Overview

LM Studio is a **local LLM platform** with an OpenAI-compatible API. It offers:
- **Zero API costs** - Free after hardware investment
- **Complete privacy** - All data stays local
- **150+ model options** - Open-source and proprietary
- **OpenAI-compatible** - Drop-in replacement for OpenAI API

## Why Use LM Studio?

### Advantages ‚úÖ
- **Privacy**: 100% local processing
- **Cost**: Free (no API fees)
- **Model variety**: 150+ models available
- **OpenAI-compatible**: Easy migration from OpenAI
- **Web UI**: User-friendly model management
- **Offline operation**: Works without internet

### Disadvantages ‚ùå
- **Hardware dependent**: Requires decent CPU/GPU
- **Setup complexity**: Must install and configure server
- **Model size**: Large downloads (10-50GB per model)
- **Speed**: Depends on your hardware

## Prerequisites

### Hardware Requirements

**Minimum** (CPU-only):
- CPU: 6+ cores
- RAM: 16GB
- Storage: 20GB per model

**Recommended** (GPU):
- GPU: NVIDIA GPU with 8GB+ VRAM
- RAM: 32GB
- Storage: SSD with 100GB+ space

### Operating System
- Windows 10/11
- macOS (Apple Silicon recommended)
- Linux

## Installation

### Step 1: Download LM Studio

1. Visit: https://lmstudio.ai
2. Click "Download" for your OS
3. Run installer
4. Launch LM Studio application

### Step 2: Download a Model

**Using LM Studio UI** (Recommended):

1. Open LM Studio
2. Click "Search Models" or go to "Models" tab
3. Browse available models
4. Click "Download" on desired model

**Popular Models**:
- `lmstudio-community/qwen2` - Qwen 2 (7B) - Fast, quality
- `lmstudio-community/Meta-Llama-3-8B` - Llama 3 (8B) - Best quality
- `lmstudio-community/Mistral-7B` - Mistral (7B) - Balanced
- `lmstudio-community/Phi-3` - Phi 3 (3.8B) - Fastest
- `lmstudio-community/CodeLlama` - Code-optimized

**Model Recommendations**:
- **Fastest**: `Phi-3` (3.8B)
- **Best Quality**: `Llama-3-8B` (8B)
- **Balanced**: `Qwen2` (7B) or `Mistral-7B` (7B)

### Step 3: Start LM Studio Server

**Using LM Studio UI**:

1. Open LM Studio
2. Go to "Server" tab
3. Click "Start Server"
4. Server runs at: `http://localhost:1234`

**Using Command Line** (Alternative):

```bash
# Windows (PowerShell)
lmstudio serve

# Windows (Run as background process)
Start-Process lmstudio "serve"

# Linux/macOS
lmstudio serve &
```

### Step 4: Verify Server

```bash
# Test server is running
curl http://localhost:1234/v1/models

# Expected: Returns JSON with available models
```

### Step 5: Test Generation

**Using LM Studio Chat UI**:
1. Go to "Chat" tab
2. Select your model
3. Type message and test

**Using API**:
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "lmstudio-community/qwen2",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": false
  }'
```

## Configuration

### Configure AI Video Factory

Edit `config.py`:

```python
# Use LM Studio as LLM provider
LLM_PROVIDER = "lmstudio"

# LM Studio server URL (default: localhost)
LMSTUDIO_BASE_URL = "http://localhost:1234"

# Model to use (must match downloaded model name)
LMSTUDIO_MODEL = "lmstudio-community/qwen2"
```

### Environment Variables (Alternative)

```bash
# Windows PowerShell
$env:LLM_PROVIDER = "lmstudio"
$env:LMSTUDIO_BASE_URL = "http://localhost:1234"
$env:LMSTUDIO_MODEL = "lmstudio-community/qwen2"

# Windows CMD
set LLM_PROVIDER=lmstudio
set LMSTUDIO_BASE_URL=http://localhost:1234
set LMSTUDIO_MODEL=lmstudio-community/qwen2

# Linux/macOS
export LLM_PROVIDER=lmstudio
export LMSTUDIO_BASE_URL=http://localhost:1234
export LMSTUDIO_MODEL=lmstudio-community/qwen2
```

## Testing

### Test 1: Verify Server Running

```bash
# Check if LM Studio is accessible
curl http://localhost:1234/v1/models

# Expected: Returns JSON with model list
```

### Test 2: Test Provider Factory

```bash
cd C:\AI\ai_video_factory
python -c "from core.llm_engine import get_provider; p = get_provider('lmstudio'); print(f'Provider: {p.name}, Model: {p.model}')"

# Expected output: Provider: LM Studio, Model: lmstudio-community/qwen2
```

### Test 3: Test Story Generation

```bash
# Set provider
set LLM_PROVIDER=lmstudio
set LMSTUDIO_MODEL=lmstudio-community/qwen2

# Test generation
python -c "from core.story_engine import build_story; print(build_story('A cat dancing')[0:200])"

# Expected: Story generated by LM Studio
```

## Troubleshooting

### "Connection refused" Error

**Cause**: LM Studio server not running

**Solution**:
1. Open LM Studio application
2. Go to "Server" tab
3. Click "Start Server"

### "Model not found" Error

**Cause**: Model not downloaded in LM Studio

**Solution**:
1. Open LM Studio
2. Go to "Models" tab
3. Search and download model
4. Verify model name matches `LMSTUDIO_MODEL` in config

### Slow Generation

**Cause**: CPU-only inference or model too large

**Solutions**:
1. **Use smaller model**: `Phi-3` (3.8B) instead of `Llama-3-70B`
2. **Enable GPU**: Install NVIDIA CUDA drivers
3. **Reduce context**: Shorter prompts = faster
4. **Use faster model**: `Qwen2` or `Mistral-7B`

### Out of Memory

**Cause**: Model too large for RAM/VRAM

**Solutions**:
1. **Use smaller model**: 7B instead of 70B
2. **Close LM Studio UI**: Run server only (saves RAM)
3. **Adjust context length**: Reduce prompt/response size

### Port Already in Use

**Cause**: Another process using port 1234

**Solution**:
```bash
# Windows - Find and kill process
netstat -ano | findstr :1234
taskkill /PID <PID> /F

# Or use different port in LM Studio Server settings
```

## Performance Tips

### GPU Acceleration

1. **Install NVIDIA drivers**: https://www.nvidia.com/Download/index.aspx
2. **LM Studio auto-detects GPU**: Check "Settings" ‚Üí "Device"
3. **Monitor GPU usage**: Check Task Manager/nvidia-smi

### Model Selection

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **Phi-3** | 3.8B | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Fastest iteration |
| **Qwen2** | 7B | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Balanced |
| **Mistral-7B** | 7B | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | General use |
| **Llama-3-8B** | 8B | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Highest quality |
| **Llama-3-70B** | 70B | ‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Best quality (slow) |

### Hardware Optimization

**For CPU-only**:
- Use smaller models (Phi-3, Qwen2)
- Close LM Studio UI (run server only)
- Use SSD for model storage
- Close background applications

**For GPU**:
- Use larger models if VRAM allows (13B, 70B)
- Monitor GPU usage with `nvidia-smi`
- Adjust context size in LM Studio settings

## Advanced Configuration

### Custom Base URL

If LM Studio running on remote server:

```python
# config.py
LMSTUDIO_BASE_URL = "http://192.168.1.100:1234"  # Remote server
```

### Model-Specific Configuration

Different models for different tasks:

```python
# For story generation (quality)
LMSTUDIO_MODEL = "lmstudio-community/Meta-Llama-3-8B"

# For rapid prototyping (speed)
LMSTUDIO_MODEL = "lmstudio-community/Phi-3"

# For code-related tasks
LMSTUDIO_MODEL = "lmstudio-community/CodeLlama-7B"
```

### Context Window

Adjust in LM Studio:
1. Settings ‚Üí Server ‚Üí Context Length
2. Default: 2048 tokens
3. Increase to 4096 or 8192 for longer contexts

## Model Library

Popular models to download in LM Studio:

```bash
# Fast & Small (3-4B)
lmstudio-community/Phi-3-mini-4k
lmstudio-community/TinyLlama-1.1B

# Balanced (7-8B)
lmstudio-community/qwen2
lmstudio-community/Mistral-7B
lmstudio-community/Meta-Llama-3-8B

# High Quality (13-70B)
lmstudio-community/Meta-Llama-3-70B
lmstudio-community/CodeLlama-13B
lmstudio-community/Mixtral-8x7B

# Specialized
lmstudio-community/CodeLlama-7B       # Code
lmstudio-community/Nous-Hermes-13B     # Chat
lmstudio-community/SpeechT5             # TTS
```

## API Reference

### LM Studio API Endpoints (OpenAI-Compatible)

**Chat Completions**:
```
POST /v1/chat/completions
{
  "model": "lmstudio-community/qwen2",
  "messages": [{"role": "user", "content": "Your prompt"}],
  "stream": false
}
```

**List Models**:
```
GET /v1/models
```

**Model Info**:
```
GET /v1/model/info?model=lmstudio-community/qwen2
```

**Embeddings**:
```
POST /v1/embeddings
{
  "model": "model-name",
  "input": "text to embed"
}
```

## Comparison with Cloud Providers

| Feature | LM Studio | OpenAI/Gemini |
|---------|-----------|--------------|
| **Cost** | Free (hardware) | Pay-per-token |
| **Privacy** | ‚≠ê‚≠ê‚≠ê (local) | ‚≠ê‚≠ê (cloud) |
| **Speed** | Hardware-dependent | Network-dependent |
| **Setup** | Moderate | Easy |
| **Offline** | ‚úÖ Yes | ‚ùå No |
| **Models** | 150+ options | Provider-specific |
| **API Format** | OpenAI-compatible | Provider-specific |

## Best Practices

### When to Use LM Studio ‚úÖ

- **Development/testing**: Cost-free iteration
- **Sensitive content**: Privacy required
- **Offline environments**: No internet available
- **Model flexibility**: Switch between 150+ models easily
- **OpenAI migration**: Drop-in replacement for OpenAI API

### When to Use Cloud Providers ‚òÅÔ∏è

- **Production**: Need guaranteed reliability
- **No GPU**: Limited local hardware
- **Best quality**: State-of-the-art cloud models
- **Simple setup**: Don't want to manage servers/models

## LM Studio vs Ollama

| Feature | LM Studio | Ollama |
|---------|-----------|--------|
| **Models** | 150+ | 20+ |
| **UI** | ‚úÖ Full GUI | ‚ùå CLI only |
| **Setup** | Easier | More manual |
| **API** | OpenAI-compatible | Custom |
| **Management** | Visual | Command-line |
| **Platform** | Win/Mac/Linux | Win/Mac/Linux |

**Choose LM Studio if**: Want user-friendly UI and more models
**Choose Ollama if**: Prefer CLI and simplicity

## Next Steps

1. ‚úÖ Install LM Studio
2. ‚úÖ Download model (via UI)
3. ‚úÖ Start server (via UI or CLI)
4. ‚úÖ Configure AI Video Factory (`config.py`)
5. ‚úÖ Test generation

**Ready to use LM Studio!** üéâ

For more information:
- LM Studio Website: https://lmstudio.ai
- LM Studio Docs: https://lmstudio.ai/docs
- Model Library: https://lmstudio.ai/models
